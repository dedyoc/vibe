services:
  # Message Streaming - Kafka in KRaft mode (no Zookeeper needed)
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
    volumes:
      - kafka_data:/tmp/kraft-combined-logs
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - streaming-network

  # Stream Processing - Flink JobManager
  flink-jobmanager:
    image: flink:1.18-scala_2.12-java11
    container_name: flink-jobmanager
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
        parallelism.default: 2
        state.backend: rocksdb
        state.checkpoints.dir: s3://flink-checkpoints/
        state.savepoints.dir: s3://flink-savepoints/
        s3.endpoint: http://minio:9000
        s3.access-key: minioadmin
        s3.secret-key: minioadmin123
        s3.path-style-access: true
    volumes:
      - flink_jobmanager_data:/opt/flink/log
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/overview"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - streaming-network
    depends_on:
      - minio

  # Stream Processing - Flink TaskManager
  flink-taskmanager:
    image: flink:1.18-scala_2.12-java11
    container_name: flink-taskmanager
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
        parallelism.default: 2
        state.backend: rocksdb
        state.checkpoints.dir: s3://flink-checkpoints/
        state.savepoints.dir: s3://flink-savepoints/
        s3.endpoint: http://minio:9000
        s3.access-key: minioadmin
        s3.secret-key: minioadmin123
        s3.path-style-access: true
    volumes:
      - flink_taskmanager_data:/opt/flink/log
    healthcheck:
      test: ["CMD", "ps", "aux", "|", "grep", "taskmanager"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - streaming-network
    depends_on:
      - flink-jobmanager
      - minio
  # Data Lake Storage - MinIO (S3-compatible)
  minio:
    image: minio/minio:RELEASE.2023-12-07T04-16-00Z
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
      MINIO_BROWSER_REDIRECT_URL: http://localhost:9001
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - streaming-network

  # MinIO Client for bucket initialization
  minio-init:
    image: minio/mc:RELEASE.2023-11-20T16-30-59Z
    container_name: minio-init
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      sleep 10;
      mc alias set minio http://minio:9000 minioadmin minioadmin123;
      mc mb minio/data-lake --ignore-existing;
      mc mb minio/flink-checkpoints --ignore-existing;
      mc mb minio/flink-savepoints --ignore-existing;
      mc policy set public minio/data-lake;
      echo 'MinIO buckets initialized successfully';
      "
    networks:
      - streaming-network

  # Caching Layer - Redis
  redis:
    image: redis:7.2-alpine
    container_name: redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s
    networks:
      - streaming-network

  # Monitoring - Prometheus
  prometheus:
    image: prom/prometheus:v2.47.2
    container_name: prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - streaming-network

  # Visualization - Grafana
  grafana:
    image: grafana/grafana:10.2.0
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin123
      GF_USERS_ALLOW_SIGN_UP: false
      GF_INSTALL_PLUGINS: redis-datasource,grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards:ro
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - streaming-network
    depends_on:
      - prometheus

  # Business Intelligence - Apache Superset
  superset:
    build:
      context: ./config/superset
      dockerfile: Dockerfile
    container_name: superset
    ports:
      - "8088:8088"
    environment:
      SUPERSET_CONFIG_PATH: /app/pythonpath/superset_config.py
      SUPERSET_HOME: /app/superset_home
      REDIS_HOST: redis
      REDIS_PORT: 6379
      SUPERSET_SECRET_KEY: bluesky-streaming-pipeline-secret-key-change-in-production
    volumes:
      - superset_data:/app/superset_home
      - ./config/superset/dashboards:/app/dashboards:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - streaming-network
    depends_on:
      - redis
      - minio

  # Application Services
  bluesky-producer:
    build:
      context: .
      dockerfile: services/bluesky-producer/Dockerfile
    container_name: bluesky-producer
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC_POSTS: bluesky-posts
      REDIS_HOST: redis
      REDIS_PORT: 6379
      LOG_LEVEL: INFO
      PROMETHEUS_PORT: 8000
    ports:
      - "8000:8000"  # Prometheus metrics endpoint
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - streaming-network
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped

  trend-processor:
    build:
      context: .
      dockerfile: services/trend-processor/Dockerfile
    container_name: trend-processor
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC_POSTS: bluesky-posts
      KAFKA_TOPIC_TRENDS: trend-alerts
      REDIS_HOST: redis
      REDIS_PORT: 6379
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin123
      MINIO_BUCKET: data-lake
      FLINK_JOBMANAGER_HOST: flink-jobmanager
      FLINK_JOBMANAGER_PORT: 8081
      LOG_LEVEL: INFO
      PROMETHEUS_PORT: 8001
    ports:
      - "8001:8001"  # Prometheus metrics endpoint
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - streaming-network
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy
      flink-jobmanager:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    restart: unless-stopped

  superset-integration:
    build:
      context: .
      dockerfile: services/superset-integration/Dockerfile
    container_name: superset-integration
    environment:
      SUPERSET_URL: http://superset:8088
      REDIS_HOST: redis
      REDIS_PORT: 6379
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin123
      LOG_LEVEL: INFO
      PROMETHEUS_PORT: 8002
    ports:
      - "8002:8002"  # Prometheus metrics endpoint
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - streaming-network
    depends_on:
      - superset
      - redis
      - minio
    restart: unless-stopped

# Persistent volumes for data storage
volumes:
  kafka_data:
    driver: local
  flink_jobmanager_data:
    driver: local
  flink_taskmanager_data:
    driver: local
  minio_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  superset_data:
    driver: local

# Network for service communication
networks:
  streaming-network:
    driver: bridge
    ipam:
      config:
        - subnet: 192.168.100.0/24